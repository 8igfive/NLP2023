{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习自然语言处理第四次作业\n",
    "\n",
    "##### 李明昕 SY2206124"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业使用 **LSTM** 实现一个 **生成式语言模型**。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 生成式语言模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用单向语言模型来建模生成式语言模型。\n",
    "\n",
    "也就是说，对于一段文本序列 $\\{w_1, w_2, \\cdots, w_n\\}$，其概率可以表示为：\n",
    "$$\n",
    "P(w_1,\\cdots,w_n)=\\prod_{i=1}^{n}P(w_i|w_{<i})\n",
    "$$\n",
    "\n",
    "对于文本生成，根据已有的文本序列 $\\{w_1,\\cdots,w_k\\}$，可以利用语言模型给出的 $P(w_{k+1}|w_{<k+1})$ 通过解码算法完成文本生成的过程。\n",
    "\n",
    "常见的解码算法包括：\n",
    "\n",
    "1. 贪心算法：在解码的过程中每次只选择概率最高的字（或词）进行生成；\n",
    "2. beam-search：维护一个大小为 $m$ 的窗口，生成的过程中保留概率前 $m$ 大的生成结果；\n",
    "3. 基于采样的算法：在生成的过程中不直接选择概率最高的字（或词）进行生成，而是通过概率进行采样生成。\n",
    "\n",
    "在解码的过程中可以通过一下三个参数调整概率分布：\n",
    "\n",
    "1. topK: 只保留分布中概率前 $K$ 大的字（或词），并对概率重新进行归一化；\n",
    "2. topP: 只保留分布中概率加和刚好超过 $P$ 的前几个字（或词），并对概率进行重新归一化；\n",
    "3. $\\tau$: 由于分布通常是通过 softmax 计算得到的，所以可以通过温度参数 $\\tau$ 改变 softmax 的结果。$\\tau$ 越大分布越均匀，$\\tau$ 越小分布越陡峭。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM（长短期记忆网络）是一种循环神经网络（RNN）的变体，特别适用于处理具有时间依赖性的序列数据。LSTM通过引入门控机制，能够有效地解决传统RNN中的长期依赖问题。LSTM的核心思想是维护一个内部记忆单元（cell state），并通过三个门控单元（输入门、遗忘门和输出门）来控制内部记忆单元的读写和遗忘操作。这些门控单元使用sigmoid函数和逐元素乘法来决定信息的流动。\n",
    "\n",
    "具体来说，对于时间步$t$：假设输入为$x(t)$，前一隐藏状态为$h(t-1)$，前一记忆单元状态为$c(t-1)$，则：\n",
    "1. 输入门（input gate）:\n",
    "   $$\n",
    "   i(t) = \\sigma(W(i) \\cdot [h(t-1), x(t)] + b(i))\n",
    "   $$\n",
    "2. 遗忘门（forget gate）:\n",
    "   $$\n",
    "   f(t) = \\sigma(W(f) \\cdot [h(t-1), x(t)] + b(f))\n",
    "   $$\n",
    "3. 输出门（output gate）:\n",
    "   $$\n",
    "   o(t) = \\sigma(W(o) \\cdot [h(t-1), x(t)] + b(o))\n",
    "   $$\n",
    "4. 新的记忆单元状态（cell state）:\n",
    "   $$\n",
    "   c'(t) = \\tanh(W(c) \\cdot [h(t-1), x(t)] + b(c))\n",
    "   $$\n",
    "5. 当前记忆单元状态（cell state）:\n",
    "   $$\n",
    "   c(t) = f(t) \\cdot c(t-1) + i(t) \\cdot c'(t)\n",
    "   $$\n",
    "6. 当前隐藏状态（output）:\n",
    "   $$\n",
    "   h(t) = o(t) \\cdot \\tanh(c(t))\n",
    "   $$\n",
    "\n",
    "在上述公式中，$W$和$b$分别表示权重和偏置项。$[h(t-1), x(t)]$表示将前一隐藏状态$h(t-1)$和当前输入$x(t)$进行拼接。$\\sigma$表示sigmoid函数，$\\tanh$表示双曲正切函数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 语料库"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库一共包含了**金庸**以及**古龙**两位武侠小说作家的共 94 部小说，其中金庸的小说共 16 部，古龙的小说共 78 部。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 数据预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先去除小说中的广告以及空白字符，然后将两位作者的小说分别整合到一个文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'resources'\n",
    "def clean_and_collect(author: str):\n",
    "    ad_p = re.compile(r\"本书来自www.cr173.com免费txt小说下载站\\n更多更新免费电子书请关注www.cr173.com\")\n",
    "    b_p = re.compile(r\"\\s\")\n",
    "    nc_p = re.compile(r\"[^\\u4e00-\\u9fa5，…：、。！？；]\")\n",
    "    \n",
    "    books = os.listdir(os.path.join(DATA_DIR, author))\n",
    "    corpus = ''\n",
    "\n",
    "    for book in books:\n",
    "        with open(os.path.join(DATA_DIR, author, book), 'r', encoding='gb2312', errors='ignore') as fi:\n",
    "            corpus += nc_p.sub('', b_p.sub('', ad_p.sub('', fi.read())))\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, author, 'corpus'), 'w', encoding='utf8') as fo:\n",
    "        fo.write(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORS = ['jinyong', 'gulong']\n",
    "for author in AUTHORS:\n",
    "    clean_and_collect(author)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 分词并构建词表"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑两种分词方法：\n",
    "\n",
    "1. 以字为单位进行分词；\n",
    "2. 通过 jieba 分词以词为单位进行分词。\n",
    "\n",
    "两种分词单位下，分词的结果如下：\n",
    "\n",
    "|分词单位|金庸小说 token 总数|古龙小说 token 总数 | 词汇表大小|\n",
    "|---|---|---|---|\n",
    "|字|8295346|16370203|5772|\n",
    "|词|5314006|10783166|292996|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenize_type: str = 'char'):\n",
    "    tokenized_corpus = {}\n",
    "    tokens = set()\n",
    "    for author in AUTHORS:\n",
    "        with open(os.path.join(DATA_DIR, author, 'corpus'), 'r', encoding='utf8') as fi:\n",
    "            if tokenize_type == 'char':\n",
    "                tokenize_fn = list\n",
    "            else:\n",
    "                tokenize_fn = lambda x: list(jieba.cut(x))\n",
    "            tokenized_corpus[author] = tokenize_fn(fi.read())\n",
    "            tokens.update(tokenized_corpus[author])\n",
    "    token2id = {token: i for i, token in enumerate(tokens)}\n",
    "    id2token = {i: token for token, i in token2id.items()}\n",
    "    return tokenized_corpus, token2id, id2token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 基于 LSTM 的生成式语言模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型主要包括三个部分：\n",
    "\n",
    "1. 嵌入层：将 token idx 所对应的 one-hot 向量转换为稠密的特征向量；\n",
    "2. LSTM 层：由多层包含 LSTM 以及前馈神经网络的模块组成；\n",
    "3. 输出层：将 token 对应的特征向量转换为此表上的 logits ，用于在 softmax 中计算概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bheveryday/software/miniconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self, embed_size: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(embed_size, embed_size, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size * 4, embed_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.l1 = nn.LayerNorm(embed_size)\n",
    "        self.l2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x, _ = self.lstm(self.l1(x))\n",
    "        x = x_res + self.lstm_dropout(x)\n",
    "        x = x + self.feedforward(self.l2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguangeModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int, layer_num: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[LSTMBlock(embed_size, dropout) for _ in range(layer_num)])\n",
    "        self.lf = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.token_embedding(idx)\n",
    "        x = self.blocks(x)\n",
    "        x = self.lf(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, _ = logits.shape\n",
    "            logits = logits.view(B*T, -1)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 模型训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 数据加载"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时，同时使用金庸以及古龙的语料进行训练。对于金庸的语料，采用顺序抽取段落的方式以保证所有文本都能得到训练；对于古龙的语料，采用随机抽取段落的方式以增加训练语料的多样性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, seq_len: int, step_interval: int, batch_size: int, tokenize_type: str = 'char', eval_p: float = 0.05):\n",
    "        self.corpus, self.token2id, self.id2token = tokenize(tokenize_type)\n",
    "        self.corpus['jinyong'], self.eval_corpus = self.corpus['jinyong'][:int(len(self.corpus['jinyong']) * (1 - eval_p))], \\\n",
    "            self.corpus['jinyong'][int(len(self.corpus['jinyong']) * (1 - eval_p)): ]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.step_interval = step_interval\n",
    "\n",
    "    def get_train_data(self):\n",
    "        batch_size_jy = int(self.batch_size * 0.7)\n",
    "        batch_size_gl = self.batch_size - batch_size_jy\n",
    "\n",
    "        jy_i = 0\n",
    "        while jy_i < len(self.corpus['jinyong']) - self.seq_len:\n",
    "            jy_is = list(range(jy_i, \n",
    "                         min(len(self.corpus['jinyong']) - self.seq_len, jy_i + batch_size_jy * self.step_interval),\n",
    "                         self.step_interval))\n",
    "            jy_i = jy_is[-1] + self.step_interval\n",
    "            gl_is = [random.randint(0, len(self.corpus['gulong']) - self.seq_len) for _ in range(batch_size_gl)]\n",
    "\n",
    "            input_ids = [[self.token2id[token] for token in self.corpus['jinyong'][left_i: left_i + self.seq_len]] \n",
    "                         for left_i in jy_is]\n",
    "            target_ids = [[self.token2id[token] for token in self.corpus['jinyong'][left_i + 1: left_i + 1 + self.seq_len]] \n",
    "                          for left_i in jy_is]\n",
    "\n",
    "            input_ids.extend([[self.token2id[token] for token in self.corpus['gulong'][left_i: left_i + self.seq_len]] \n",
    "                              for left_i in gl_is])\n",
    "            target_ids.extend([[self.token2id[token] for token in self.corpus['gulong'][left_i + 1: left_i + 1 + self.seq_len]] \n",
    "                               for left_i in gl_is])\n",
    "            \n",
    "            yield torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil((len(self.corpus['jinyong']) - self.seq_len) // self.step_interval / int(self.batch_size * 0.7))\n",
    "\n",
    "    def get_eval_data(self):\n",
    "        return torch.tensor([self.token2id[token] for token in self.eval_corpus], dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from preprocess import tokenize\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, seq_len: int, step_interval: int, batch_size: int, tokenize_type: str = 'char', eval_p: float = 0.05):\n",
    "        self.corpus, self.token2id, self.id2token = tokenize(tokenize_type)\n",
    "        self.corpus['jinyong'], self.eval_corpus = self.corpus['jinyong'][:int(len(self.corpus['jinyong']) * (1 - eval_p))], \\\n",
    "            self.corpus['jinyong'][int(len(self.corpus['jinyong']) * (1 - eval_p)): ]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.step_interval = step_interval\n",
    "\n",
    "    def get_train_data(self):\n",
    "        batch_size_jy = int(self.batch_size * 0.7)\n",
    "        batch_size_gl = self.batch_size - batch_size_jy\n",
    "\n",
    "        jy_i = 0\n",
    "        while jy_i < len(self.corpus['jinyong']) - self.seq_len:\n",
    "            jy_is = list(range(jy_i, \n",
    "                         min(len(self.corpus['jinyong']) - self.seq_len, jy_i + batch_size_jy * self.step_interval),\n",
    "                         self.step_interval))\n",
    "            jy_i = jy_is[-1] + self.step_interval\n",
    "            gl_is = [random.randint(0, len(self.corpus['gulong']) - self.seq_len) for _ in range(batch_size_gl)]\n",
    "\n",
    "            input_ids = [[self.token2id[token] for token in self.corpus['jinyong'][left_i: left_i + self.seq_len]] \n",
    "                         for left_i in jy_is]\n",
    "            target_ids = [[self.token2id[token] for token in self.corpus['jinyong'][left_i + 1: left_i + 1 + self.seq_len]] \n",
    "                          for left_i in jy_is]\n",
    "\n",
    "            input_ids.extend([[self.token2id[token] for token in self.corpus['gulong'][left_i: left_i + self.seq_len]] \n",
    "                              for left_i in gl_is])\n",
    "            target_ids.extend([[self.token2id[token] for token in self.corpus['gulong'][left_i + 1: left_i + 1 + self.seq_len]] \n",
    "                               for left_i in gl_is])\n",
    "            \n",
    "            yield torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil((len(self.corpus['jinyong']) - self.seq_len) // self.step_interval / int(self.batch_size * 0.7))\n",
    "\n",
    "    def get_eval_data(self):\n",
    "        return torch.tensor([self.token2id[token] for token in self.eval_corpus], dtype=torch.long)\n",
    "    \n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, corpus, epoch, lr, wu_steps, device):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.model = model.to(device=device)\n",
    "        self.corpus = corpus\n",
    "        self.step_per_epoch = len(self.corpus)\n",
    "        self.epoch = epoch\n",
    "        self.min_lr, self.max_lr = lr\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.max_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer,\n",
    "            lr_lambda=lambda cur_iter: cur_iter / wu_steps if cur_iter < wu_steps else\n",
    "                (self.min_lr + 0.5*(self.max_lr - self.min_lr) * \n",
    "                 (1 + math.cos(((cur_iter - wu_steps) / (self.step_per_epoch * epoch - wu_steps) * math.pi)))) / \n",
    "                 self.max_lr\n",
    "        )\n",
    "        self.loss_log = []\n",
    "\n",
    "    def train(self, check_interval=100):\n",
    "        global_step = 0\n",
    "        min_loss = float('inf')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            for input_ids, target_ids in self.corpus.get_train_data():\n",
    "                input_ids = input_ids.to(device=self.device)\n",
    "                target_ids = target_ids.to(device=self.device)\n",
    "\n",
    "                _, loss = self.model(input_ids, target_ids)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.scheduler.step()\n",
    "\n",
    "                self.loss_log.append(loss.item())\n",
    "                global_step += 1\n",
    "                if global_step % check_interval == 0:\n",
    "                    if self.loss_log[-1] < min_loss:\n",
    "                        suffix = \", model saved to resources/ckpt/min_loss.ckpt\"\n",
    "                        torch.save(self.model.state_dict(), 'resources/ckpt/min_loss.ckpt')\n",
    "                    else:\n",
    "                        suffix = ''\n",
    "                    step_time = (time.time() - start_time) / global_step\n",
    "                    print(f'Epoch: {epoch}, GlobalStep: {global_step}, lr: {self.scheduler.get_last_lr()[0]:.5f}, eta: {step_time * (self.step_per_epoch * self.epoch - global_step) / 3600:.3f}h, loss: {self.loss_log[-1]:.4f}{suffix}')\n",
    "            torch.save(self.model.state_dict(), f'resources/ckpt/{epoch}.ckpt')\n",
    "            print(f'Save Model of Epoch: {epoch} to resources/ckpy/{epoch}.ckpt')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 实验"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 以字为单位进行分词得到的语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "corpus = Corpus(128, 16, 64, 'char', 0.05)\n",
    "model = LSTMLanguangeModel(len(corpus.token2id), 384, 12, 0.2)\n",
    "trainer = Trainer(model, corpus, 2, (5e-5, 1e-4), 1000, torch.device('cuda:0'))\n",
    "\n",
    "# trainer.train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
